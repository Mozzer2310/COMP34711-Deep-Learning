{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mozzer2310/COMP34711-Deep-Learning/blob/main/task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qifa7JfDtPM",
        "outputId": "66ae21f4-a91f-4b3d-9d09-897cfb189fe6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-07 17:59:27.854619: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-12-07 17:59:28.434090: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
            "2022-12-07 17:59:28.434144: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
            "2022-12-07 17:59:28.434149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.vocab = set()\n",
        "        self.reviews = []\n",
        "        self.classification = []\n",
        "\n",
        "    def read_data(self, path: str) -> list:\n",
        "        # Find all the .txt files at the path, remove the README from the list\n",
        "        file_paths = glob.glob(path + \"/*.txt\")\n",
        "        file_paths.remove(path + \"/README.txt\")\n",
        "\n",
        "        corpora = []\n",
        "        # Read each file in the list of files\n",
        "        for file_path in file_paths:\n",
        "            f = open(file_path, \"r\")\n",
        "            # Add the data to an array of corpora\n",
        "            corpora.append(f.read())\n",
        "\n",
        "        return corpora\n",
        "\n",
        "    def preprocess(self, corpora: list):\n",
        "        self.reviews = []\n",
        "        self.classification = []\n",
        "        # process the raw data of each corpus in the list\n",
        "        for corpus in corpora:\n",
        "            self.process_raw(corpus)\n",
        "\n",
        "    def process_raw(self, raw: str):\n",
        "        # split over the lines (## defines a line and is on each new line as defined by README)\n",
        "        lines = raw.splitlines()\n",
        "        # remove '[t]' tags\n",
        "        lines = [ele for ele in lines if ele != \"[t]\"]\n",
        "\n",
        "        # process each line in the text, add the result to an array and add review class to an array\n",
        "        for line in lines:\n",
        "            # Check that the line isn't empty\n",
        "            if len(line) != 0:\n",
        "                # Process the line, get returned processed line and its review info for classifying\n",
        "                processed_review, review_info = self.process_line(line)\n",
        "                # Only consider reviews which can be classified, i.e. have been classified in text file\n",
        "                if len(review_info) != 0:\n",
        "                    # Consider weights of reviews, in the case that a review is part positive and part negative\n",
        "                    # the weights will help when classifying a review if it is 'more' postive than negative, and vice versa\n",
        "                    num_pos = 3 * review_info.count(\"+3\") + 2 * review_info.count(\n",
        "                        \"+2\") + review_info.count(\"+1\") + review_info.count(\"+\")\n",
        "                    num_neg = 3 * review_info.count(\"-3\") + 2 * review_info.count(\n",
        "                        \"-2\") + review_info.count(\"-1\") + review_info.count(\"-\")\n",
        "                    # 1 for postive and 0 for negative review, add to list\n",
        "                    if num_pos > num_neg:\n",
        "                        self.classification.append(1)\n",
        "                        # add the review to an array\n",
        "                        self.reviews.append(processed_review)\n",
        "                    elif num_pos < num_neg:\n",
        "                        self.classification.append(0)\n",
        "                        # add the review to an array\n",
        "                        self.reviews.append(processed_review)\n",
        "\n",
        "    def process_line(self, line: str):\n",
        "        # Get the substring before the ## delimiter, if not present return empty values for error handling\n",
        "        try:\n",
        "            delim_index = line.index(\"##\")\n",
        "        except ValueError:\n",
        "            delim_index = None\n",
        "        if delim_index == None:\n",
        "            return [], \"\"\n",
        "        # sub-string before the delimiter is the information about the class of review\n",
        "        review_info = line[:delim_index]\n",
        "        # sub-string after the delimiter is the review\n",
        "        line = line[delim_index+2:]\n",
        "\n",
        "        return line, review_info\n",
        "\n",
        "    def test(self):\n",
        "        train_list_reviews = self.reviews[:1800]\n",
        "        train_list_class = self.classification[:1800]\n",
        "        test_list_reviews = self.reviews[1800:]\n",
        "        test_list_class = self.classification[1800:]\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (train_list_reviews, train_list_class))\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (test_list_reviews, test_list_class))\n",
        "        print(train_dataset)\n",
        "        print(test_dataset)\n",
        "\n",
        "        BUFFER_SIZE = 10000\n",
        "        BATCH_SIZE = 32\n",
        "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(\n",
        "            BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "        test_dataset = test_dataset.batch(\n",
        "            BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        # for example, label in train_dataset.take(1):\n",
        "        #     print('texts: ', example.numpy()[:3])\n",
        "        #     print()\n",
        "        #     print('labels: ', label.numpy()[:3])\n",
        "\n",
        "        VOCAB_SIZE = 5000\n",
        "        encoder = tf.keras.layers.TextVectorization(\n",
        "            max_tokens=VOCAB_SIZE)\n",
        "        encoder.adapt(train_dataset.map(lambda text, label: text))\n",
        "\n",
        "        vocab = np.array(encoder.get_vocabulary())\n",
        "        print(vocab[:20])\n",
        "\n",
        "        model = tf.keras.Sequential([\n",
        "            encoder,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=len(encoder.get_vocabulary()),\n",
        "                output_dim=32,\n",
        "                # Use masking to handle the variable sequence lengths\n",
        "                mask_zero=True),\n",
        "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        # model = tf.keras.Sequential([\n",
        "        #     encoder,\n",
        "        #     tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 32, mask_zero=True),\n",
        "        #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,  return_sequences=True)),\n",
        "        #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "        #     tf.keras.layers.Dense(32, activation='relu'),\n",
        "        #     tf.keras.layers.Dropout(0.5),\n",
        "        #     tf.keras.layers.Dense(1)\n",
        "        # ])\n",
        "\n",
        "        model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                      optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        history = model.fit(train_dataset, epochs=10,\n",
        "                            validation_data=test_dataset,\n",
        "                            validation_steps=10)\n",
        "\n",
        "        test_loss, test_acc = model.evaluate(test_dataset)\n",
        "        print(test_acc)\n",
        "\n",
        "        # positive = (\"The arm band is fantastic and it doesn't budge even at the gym.\")\n",
        "        # negative = (\"This router was a huge disapointment.\")\n",
        "        # predictions = model.predict(np.array([positive, negative]))\n",
        "        # print(predictions)\n",
        "\n",
        "    def nfold_cv(self, n: int = 5):\n",
        "        pos_inds = np.where(np.array(self.classification) == 1)\n",
        "        neg_inds = np.where(np.array(self.classification) == 0)\n",
        "\n",
        "        pos_reviews = list(np.array(self.reviews)[pos_inds])\n",
        "        neg_reviews = list(np.array(self.reviews)[neg_inds])\n",
        "        print(pos_reviews)\n",
        "        print(neg_reviews)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2UMfC-JET62",
        "outputId": "cce9111f-37b6-4fa8-abc7-ceaf9554a295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2094\n",
            "2094\n",
            "1351\n",
            "743\n",
            "270\n",
            "148\n",
            "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n",
            "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n",
            "WARNING:tensorflow:From /home/mozzer/.local/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "['' '[UNK]' 'the' 'and' 'to' 'i' 'a' 'it' 'is' 'of' 'this' 'with' 'you'\n",
            " 'for' 'that' 'in' 'have' 'but' 'my' 'not']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-07 17:59:29.059831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-07 17:59:29.077817: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
            "2022-12-07 17:59:29.077837: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-12-07 17:59:29.078135: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "57/57 [==============================] - 6s 33ms/step - loss: 0.6897 - accuracy: 0.3567 - val_loss: 0.6840 - val_accuracy: 0.3435\n",
            "Epoch 2/10\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.6787 - accuracy: 0.3567 - val_loss: 0.6695 - val_accuracy: 0.3435\n",
            "Epoch 3/10\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.6636 - accuracy: 0.3567 - val_loss: 0.6507 - val_accuracy: 0.3435\n",
            "Epoch 4/10\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.6488 - accuracy: 0.4933 - val_loss: 0.6391 - val_accuracy: 0.6122\n",
            "Epoch 5/10\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.6364 - accuracy: 0.6350 - val_loss: 0.6301 - val_accuracy: 0.6701\n",
            "Epoch 6/10\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.6159 - accuracy: 0.7472 - val_loss: 0.6143 - val_accuracy: 0.6973\n",
            "Epoch 7/10\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.5740 - accuracy: 0.7872 - val_loss: 0.5808 - val_accuracy: 0.7279\n",
            "Epoch 8/10\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.4914 - accuracy: 0.8472 - val_loss: 0.5887 - val_accuracy: 0.7211\n",
            "Epoch 9/10\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.4171 - accuracy: 0.8744 - val_loss: 0.5478 - val_accuracy: 0.6837\n",
            "Epoch 10/10\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.3478 - accuracy: 0.8944 - val_loss: 0.5802 - val_accuracy: 0.7041\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.5802 - accuracy: 0.7041\n",
            "0.704081654548645\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    neural = NeuralNetwork()\n",
        "    # specify the directory path to the review files\n",
        "    corpora = neural.read_data(\"product_reviews\")\n",
        "\n",
        "    neural.preprocess(corpora)\n",
        "    print(len(neural.reviews))\n",
        "    print(len(neural.classification))\n",
        "    print(neural.classification.count(1))\n",
        "    print(neural.classification.count(0))\n",
        "    print(neural.classification.count(1)//5)\n",
        "    print(neural.classification.count(0)//5)\n",
        "    # neural.nfold_cv()\n",
        "    neural.test()\n",
        "\n",
        "\n",
        "test = main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOCktKHMHORX6BJmdTrZ2yy",
      "include_colab_link": true,
      "mount_file_id": "1XFe9hwn8zEsXgooZ5ES7sHPFC64mvjdq",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
