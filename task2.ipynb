{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mozzer2310/COMP34711-Deep-Learning/blob/main/task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBdrLWZC7YdO",
        "outputId": "3eb9d8a5-abc5-4815-fbd2-9ee4eba288c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Colab generated code to mount drive, remove/comment out if not needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qifa7JfDtPM",
        "outputId": "58959f15-418b-4ee1-9126-8c002321fd10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/mozzer/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "# Download nltk punkt for tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.vocab = set()\n",
        "        self.reviews = []\n",
        "        self.classification = []\n",
        "\n",
        "    def read_data(self, path: str) -> list:\n",
        "        # Find all the .txt files at the path, remove the README from the list\n",
        "        file_paths = glob.glob(path + \"/*.txt\")\n",
        "        file_paths.remove(path + \"/README.txt\")\n",
        "\n",
        "        corpora = []\n",
        "        # Read each file in the list of files\n",
        "        for file_path in file_paths:\n",
        "            f = open(file_path, \"r\")\n",
        "            # Add the data to an array of corpora\n",
        "            corpora.append(f.read())\n",
        "\n",
        "        return corpora\n",
        "\n",
        "    def preprocess(self, corpora: list):\n",
        "        self.reviews = []\n",
        "        self.classification = []\n",
        "        # process the raw data of each corpus in the list\n",
        "        for corpus in corpora:\n",
        "            self.process_raw(corpus)\n",
        "\n",
        "        # Flatten the processed reviews, to get a single list, convert to a set to get the vocab list\n",
        "        self.vocab = set(\n",
        "            [item for sublist in self.reviews for item in sublist.split(\" \")])\n",
        "\n",
        "    def process_raw(self, raw: str):\n",
        "        # split over the lines (## defines a line and is on each new line as defined by README)\n",
        "        lines = raw.splitlines()\n",
        "        # remove '[t]' tags\n",
        "        lines = [ele for ele in lines if ele != \"[t]\"]\n",
        "\n",
        "        # process each line in the text, add the result to an array and add review class to an array\n",
        "        for line in lines:\n",
        "            # Check that the line isn't empty\n",
        "            if len(line) != 0:\n",
        "                # Process the line, get returned processed line and its review info for classifying\n",
        "                processed_review, review_info = self.process_line(line)\n",
        "                # Only consider reviews which can be classified, i.e. have been classified in text file\n",
        "                if len(review_info) != 0:\n",
        "                    # Consider weights of reviews, in the case that a review is part positive and part negative\n",
        "                    # the weights will help when classifying a review if it is 'more' postive than negative, and vice versa\n",
        "                    num_pos = 3 * review_info.count(\"+3\") + 2 * review_info.count(\n",
        "                        \"+2\") + review_info.count(\"+1\") + review_info.count(\"+\")\n",
        "                    num_neg = 3 * review_info.count(\"-3\") + 2 * review_info.count(\n",
        "                        \"-2\") + review_info.count(\"-1\") + review_info.count(\"-\")\n",
        "                    # 1 for postive and 0 for negative review, add to list\n",
        "                    if num_pos > num_neg:\n",
        "                        self.classification.append(1)\n",
        "                        # add the review to an array\n",
        "                        self.reviews.append(processed_review)\n",
        "                    elif num_pos < num_neg:\n",
        "                        self.classification.append(0)\n",
        "                        # add the review to an array\n",
        "                        self.reviews.append(processed_review)\n",
        "\n",
        "    def process_line(self, line: str):\n",
        "        # Get the substring before the ## delimiter, if not present return empty values for error handling\n",
        "        try:\n",
        "            delim_index = line.index(\"##\")\n",
        "        except ValueError:\n",
        "            delim_index = None\n",
        "        if delim_index == None:\n",
        "            return [], \"\"\n",
        "        # sub-string before the delimiter is the information about the class of review\n",
        "        review_info = line[:delim_index]\n",
        "        # sub-string after the delimiter is the review\n",
        "        line = line[delim_index+2:]\n",
        "\n",
        "        # Convert to lower case\n",
        "        line_lwr = line.lower()\n",
        "        # # Remove everything except punctuation\n",
        "        line_clean = line_lwr.translate(\n",
        "            str.maketrans('', '', string.punctuation))\n",
        "        # Tokenize the review and rejoin to remove whitespace\n",
        "        line_tokens = \" \".join(word_tokenize(line_clean))\n",
        "\n",
        "        return line_tokens, review_info\n",
        "\n",
        "    def build_train_model(self,\n",
        "                          train_data: list,\n",
        "                          train_class: list,\n",
        "                          test_data: list,\n",
        "                          test_class: list,\n",
        "                          RECURRENT_DROPOUT: float = 0.2,\n",
        "                          BATCH_SIZE: int = 16):\n",
        "        # convert the train and test data and classes to a tensor flow dataset\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (train_data, train_class))\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (test_data, test_class))\n",
        "\n",
        "        # Create batches of the train and test data\n",
        "        # to be passed to the model\n",
        "        BUFFER_SIZE = 10000\n",
        "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(\n",
        "            BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "        test_dataset = test_dataset.batch(\n",
        "            BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        # Set a vocab size\n",
        "        VOCAB_SIZE = 5000\n",
        "        # set the encoding layer to keras TextVectorization\n",
        "        encoder = tf.keras.layers.TextVectorization(\n",
        "            max_tokens=VOCAB_SIZE)\n",
        "        # Set the layers vocab from the dataset text data\n",
        "        encoder.adapt(train_dataset.map(lambda text, label: text))\n",
        "\n",
        "        # Create the model\n",
        "        # encoding layer\n",
        "        # embedding layer, input dimension the vocab size\n",
        "        # bidirection LSTM layer with dropout defined on input\n",
        "        # Converts the output of the LSTM to a single value, activation RELU\n",
        "        model = tf.keras.Sequential([\n",
        "            encoder,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=len(encoder.get_vocabulary()),\n",
        "                output_dim=32,\n",
        "                # Use masking to handle the variable sequence lengths\n",
        "                mask_zero=True),\n",
        "            tf.keras.layers.Bidirectional(\n",
        "                tf.keras.layers.LSTM(32, recurrent_dropout=RECURRENT_DROPOUT)),\n",
        "            # tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='relu')\n",
        "        ])\n",
        "\n",
        "        # Compile the model to configure the training process\n",
        "        model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                      optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # train the model on the train data set with 10 epochs and no output\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            epochs=10,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            verbose=0)\n",
        "\n",
        "        # Evaluate the model on the test dataset\n",
        "        test_loss, test_acc = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "        return test_acc\n",
        "\n",
        "    def nfold_cv(self, n: int = 5, recurrent_dropout: float = 0.2, batch_size: int = 16) -> list:\n",
        "        # get the indices of the positve and negative reviews seperately\n",
        "        pos_inds = np.where(np.array(self.classification) == 1)\n",
        "        neg_inds = np.where(np.array(self.classification) == 0)\n",
        "\n",
        "        # get the postive and negative reviews in separate lists\n",
        "        pos_reviews = list(np.array(self.reviews)[pos_inds])\n",
        "        neg_reviews = list(np.array(self.reviews)[neg_inds])\n",
        "\n",
        "        # shuffle the lists randomly\n",
        "        shuffled_pos = pos_reviews.copy()\n",
        "        shuffled_neg = neg_reviews.copy()\n",
        "        random.shuffle(shuffled_pos)\n",
        "        random.shuffle(shuffled_neg)\n",
        "\n",
        "        # instantiate empty lists in a dict\n",
        "        groups = {\"test\": [], \"test_class\": [], \"train\": [], \"train_class\": []}\n",
        "        # for the number of folds\n",
        "        for split in range(n):\n",
        "            # get the lower position of the positive and negative reviews\n",
        "            lower_pos = (len(shuffled_pos)//n) * split\n",
        "            lower_neg = (len(shuffled_neg)//n) * split\n",
        "            # edge case, for when upper is not needed (at end of lists)\n",
        "            if split == n-1:\n",
        "                # split the two lists into testing (from lower bound to end of list)\n",
        "                # and training (the rest of the lists)\n",
        "                test_pos = shuffled_pos[lower_pos:]\n",
        "                test_neg = shuffled_neg[lower_neg:]\n",
        "                train_pos = shuffled_pos[:lower_pos]\n",
        "                train_neg = shuffled_neg[:lower_neg]\n",
        "            else:\n",
        "                # split the two lists into testing (between the two bounds)\n",
        "                # and training (the rest of the lists)\n",
        "                upper_pos = (len(shuffled_pos)//n) * (split+1)\n",
        "                upper_neg = (len(shuffled_neg)//n) * (split+1)\n",
        "                test_pos = shuffled_pos[lower_pos:upper_pos]\n",
        "                test_neg = shuffled_neg[lower_neg:upper_neg]\n",
        "                train_pos = shuffled_pos[:lower_pos] + shuffled_pos[upper_pos:]\n",
        "                train_neg = shuffled_neg[:lower_neg] + shuffled_neg[upper_neg:]\n",
        "            # add the relevant sections of the lists to the dict\n",
        "            groups[\"test\"].append(test_pos + test_neg)\n",
        "            groups[\"train\"].append(train_pos + train_neg)\n",
        "            # create two class list the with 1s for positive and 0s for negative\n",
        "            groups[\"test_class\"].append(\n",
        "                [1] * len(test_pos) + [0] * len(test_neg))\n",
        "            groups[\"train_class\"].append(\n",
        "                [1] * len(train_pos) + [0] * len(train_neg))\n",
        "\n",
        "        accuracies = []\n",
        "        # pass the data for each fold to the model for training and testing\n",
        "        for fold in range(n):\n",
        "            accuracies.append(self.build_train_model(groups[\"train\"][fold],\n",
        "                                                     groups[\"train_class\"][fold],\n",
        "                                                     groups[\"test\"][fold],\n",
        "                                                     groups[\"test_class\"][fold],\n",
        "                                                     RECURRENT_DROPOUT=recurrent_dropout,\n",
        "                                                     BATCH_SIZE=batch_size))\n",
        "\n",
        "        return accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2UMfC-JET62",
        "outputId": "05474592-12b0-49ea-d02d-a4aeb7977530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyper-parameter (Recurrent Dropout) Optimisation Results:\n",
            "Recurrent Dropout: 0, Accuracy: 0.7463593363761902\n",
            "Recurrent Dropout: 0.1, Accuracy: 0.7340188980102539\n",
            "Recurrent Dropout: 0.2, Accuracy: 0.7444681406021119\n",
            "Recurrent Dropout: 0.3, Accuracy: 0.7392094850540161\n",
            "Recurrent Dropout: 0.4, Accuracy: 0.7540647149085998\n",
            "Recurrent Dropout: 0.5, Accuracy: 0.7507199764251709\n",
            "Hyper-parameter (Batch size) Optimisation Results:\n",
            "Batch Size: 8, Accuracy: 0.7397605419158936\n",
            "Batch Size: 16, Accuracy: 0.743511188030243\n",
            "Batch Size: 32, Accuracy: 0.7301820755004883\n",
            "Batch Size: 48, Accuracy: 0.667575228214264\n",
            "Batch Size: 64, Accuracy: 0.6094333171844483\n",
            "\n",
            "Average results for review classification over 5 folds with batch size: 16 and recurrent dropout: 0.2\n",
            "Accuracies: [0.739234447479248, 0.7320573925971985, 0.7248803973197937, 0.7631579041481018, 0.7109004855155945]\n",
            "Mean of Accuracies: 0.7340461254119873\n",
            "Standard Deviation of Accuracies: 0.017309538543451984\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    neural = NeuralNetwork()\n",
        "    # specify the directory path to the review files\n",
        "    corpora = neural.read_data(\"product_reviews\")\n",
        "\n",
        "    neural.preprocess(corpora)\n",
        "    hyper_recur = input(\n",
        "        \"Do you want to do hyper parameter selection for the recurrent dropout? yes or no\\nNote: takes around 8-10 mins on my system to run, if not the default will be my usual hyper-parameter: 0.2\")\n",
        "    if (hyper_recur == \"yes\"):\n",
        "        recur_dropout = hyper_parameter_select(neural=neural, recur_dropouts=[\n",
        "                                               0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "    else:\n",
        "        recur_dropout = 0.2\n",
        "    # Set recurrent dropout to 0.2 regardless, as from my results there isn't much difference with these values\n",
        "    recur_dropout = 0.2\n",
        "\n",
        "    hyper_batch = input(\n",
        "        \"Do you want to do hyper parameter selection for the batch size? yes or no\\nNote: takes around 8-10 mins on my system to run, if not the default will be my usual hyper-parameter: 16\")\n",
        "    if (hyper_batch == \"yes\"):\n",
        "        batch_size = hyper_parameter_select(\n",
        "            neural=neural, batch_sizes=[8, 16, 32, 48, 64])\n",
        "    else:\n",
        "        batch_size = 16\n",
        "\n",
        "    FOLDS = 5\n",
        "    accuracies = neural.nfold_cv(\n",
        "        n=FOLDS, batch_size=batch_size, recurrent_dropout=recur_dropout)\n",
        "    print(\n",
        "        f\"\\nAverage results for review classification over {FOLDS} folds with batch size: {batch_size} and recurrent dropout: {recur_dropout}\")\n",
        "    print(f\"Accuracies: {accuracies}\")\n",
        "    print(f\"Mean of Accuracies: {np.mean(accuracies)}\")\n",
        "    print(f\"Standard Deviation of Accuracies: {np.std(accuracies)}\")\n",
        "\n",
        "\n",
        "def hyper_parameter_select(neural: NeuralNetwork, recur_dropouts: list = [], batch_sizes: list = []) -> int:\n",
        "    # hyper-parameter selection of recurrent_dropout and batches\n",
        "\n",
        "    # Case where the hyper-parameters are being optimised at the same time\n",
        "    if len(recur_dropouts) != 0 and len(batch_sizes) != 0:\n",
        "        # Combine the lists so all combinations are obtained\n",
        "        comb = product(recur_dropouts, batch_sizes)\n",
        "        comb = list(comb)\n",
        "        results = []\n",
        "        # Run 5 fold (default) CV for each combination\n",
        "        for recur_dropout, batch_size in comb:\n",
        "            accuracies = neural.nfold_cv(\n",
        "                recurrent_dropout=recur_dropout, batch_size=batch_size)\n",
        "            results.append(np.mean(accuracies))\n",
        "\n",
        "        # Work out the recurrent dropout and batch size with the best accuracy and return that\n",
        "        # Display the resulting accuracies for each combination\n",
        "        max_ind = -1\n",
        "        max = 0\n",
        "        i = 0\n",
        "        print(f\"Hyper-parameter (Recurrent Dropout & Batch Size) Optimisation Results:\")\n",
        "        for recur_dropout, batch_size in comb:\n",
        "            if results[i] > max:\n",
        "                max = results[i]\n",
        "                max_ind = i\n",
        "            print(\n",
        "                f\"Recurrent Dropout: {recur_dropout}, Batch Size: {batch_size}, Accuracy: {results[i]}\")\n",
        "            i += 1\n",
        "\n",
        "        return (recur_dropouts[max_ind])\n",
        "    # Case where just recurrent dropout is being optimised\n",
        "    elif len(recur_dropouts) != 0:\n",
        "        results = []\n",
        "        for recur_dropout in recur_dropouts:\n",
        "            accuracies = neural.nfold_cv(recurrent_dropout=recur_dropout)\n",
        "            results.append(np.mean(accuracies))\n",
        "\n",
        "        # Work out the recurrent dropout with the best accuracy and return that\n",
        "        # Display the resulting accuracies for each value\n",
        "        max_ind = -1\n",
        "        max = 0\n",
        "        print(f\"Hyper-parameter (Recurrent Dropout) Optimisation Results:\")\n",
        "        for i in range(len(recur_dropouts)):\n",
        "            if results[i] > max:\n",
        "                max = results[i]\n",
        "                max_ind = i\n",
        "            print(\n",
        "                f\"Recurrent Dropout: {recur_dropouts[i]}, Accuracy: {results[i]}\")\n",
        "\n",
        "        return (recur_dropouts[max_ind])\n",
        "    # Case where just batch size is being optimised\n",
        "    else:\n",
        "        results = []\n",
        "        for batch_size in batch_sizes:\n",
        "            accuracies = neural.nfold_cv(batch_size=batch_size)\n",
        "            results.append(np.mean(accuracies))\n",
        "\n",
        "        # Work out the batch size with the best accuracy and return that\n",
        "        # Display the resulting accuracies for each value\n",
        "        max_ind = -1\n",
        "        max = 0\n",
        "        print(f\"Hyper-parameter (Batch size) Optimisation Results:\")\n",
        "        for i in range(len(batch_sizes)):\n",
        "            if results[i] > max:\n",
        "                max = results[i]\n",
        "                max_ind = i\n",
        "            print(f\"Batch Size: {batch_sizes[i]}, Accuracy: {results[i]}\")\n",
        "\n",
        "        return (batch_sizes[max_ind])\n",
        "\n",
        "\n",
        "test = main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
