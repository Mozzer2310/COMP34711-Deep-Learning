{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mozzer2310/COMP34711-Deep-Learning/blob/main/task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhxcHDJJAMSM",
        "outputId": "93ae5cb2-26b1-4594-ee71-41b61ca921be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Colab generated code to mount drive, remove/comment out if not needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3spzRKfdAJ-b",
        "outputId": "3dabb699-b028-4bbc-d0a3-af2f9fb4584e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import random\n",
        "import re\n",
        "import copy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import numpy as np\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from numpy.linalg import svd, matrix_rank\n",
        "\n",
        "# Download nltk punkt for tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "class DistributionalSemantics:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.vocab = set()\n",
        "        self.docs = []\n",
        "        self.docs_sampled = []\n",
        "        self.target_words = []\n",
        "        self.psuedo_words = []\n",
        "\n",
        "    def read_data(self, path: str) -> list:\n",
        "        # Find all the .txt files at the path, remove the README from the list\n",
        "        file_paths = glob.glob(path + \"/*.txt\")\n",
        "        file_paths.remove(path + \"/README.txt\")\n",
        "\n",
        "        corpora = []\n",
        "        # Read each file in the list of files\n",
        "        for file_path in file_paths:\n",
        "            f = open(file_path, \"r\")\n",
        "            # Add the data to an array of corpora\n",
        "            corpora.append(f.read())\n",
        "\n",
        "        return corpora\n",
        "\n",
        "    def preprocess(self, corpora: list):\n",
        "        self.docs = []\n",
        "        # process the raw data of each corpus in the list\n",
        "        for corpus in corpora:\n",
        "            self.process_raw(corpus)\n",
        "\n",
        "        # Flatten the processed docs, to get a single list, convert to a set to get the vocab list\n",
        "        self.vocab = set(\n",
        "            [item for sublist in self.docs for item in sublist])\n",
        "\n",
        "    def process_raw(self, raw: str):\n",
        "        stop_words = set(ENGLISH_STOP_WORDS).copy()\n",
        "        # Add stopwords not in sklearn list, these would appear in top 50 list otherwise\n",
        "        stop_words.update([\"ive\", \"im\", \"dont\"])\n",
        "\n",
        "        # remove [t] tag lines\n",
        "        lines = raw.splitlines()\n",
        "        lines = [line for line in lines if line not in [\"[t]\"]]\n",
        "\n",
        "        stemmed_doc = []\n",
        "        for line in lines:\n",
        "            # find the position in the string of the delimiter '##'\n",
        "            # remove the content before the delimiter and the delimiter itself\n",
        "            try:\n",
        "                delim_index = line.index(\"##\")\n",
        "                line = line[delim_index+2:]\n",
        "            # If delimiter not present the line is okay to process\n",
        "            except ValueError:\n",
        "                line = line\n",
        "\n",
        "            # Convert to lower case\n",
        "            line_lwr = line.lower()\n",
        "            # Remove everything except alpha characters, numbers, and whitespace\n",
        "            line_clean = re.sub(r'[^a-z0-9\\s]+', '', line_lwr)\n",
        "            # Tokenize the document\n",
        "            line_tokens = word_tokenize(line_clean)\n",
        "            # Remove stopwords\n",
        "            filtered_docs = [w for w in line_tokens if w not in stop_words]\n",
        "\n",
        "            # Stemming with Snowball Stemmer\n",
        "            snow_stemmer = SnowballStemmer(language='english')\n",
        "            stemmed_doc.extend([snow_stemmer.stem(word)\n",
        "                                for word in filtered_docs])\n",
        "\n",
        "        self.docs.append(stemmed_doc)\n",
        "\n",
        "    def find_target_words(self):\n",
        "        # Flatten list of docs into one list\n",
        "        words = [item for sublist in self.docs for item in sublist]\n",
        "        vocab_occurance = []\n",
        "        # Count the number of occurances of each word in the vocab\n",
        "        for word in self.vocab:\n",
        "            vocab_occurance.append(words.count(word))\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        np_vocab_occurnace = np.array(vocab_occurance)\n",
        "        np_vocab = np.array(list(self.vocab))\n",
        "        # get the indices from argsort of the number of occurances in descending order\n",
        "        inds = np_vocab_occurnace.argsort()[::-1]\n",
        "\n",
        "        # Sort the vocab list by the occurances (from indices) get top 50 results\n",
        "        self.target_words = list(np_vocab[inds][:50])\n",
        "        # Reverse target words to get psuedo_words\n",
        "        self.psuedo_words = [word[::-1] for word in self.target_words]\n",
        "\n",
        "        # add psuedo_words to vocab\n",
        "        self.vocab.update(self.psuedo_words)\n",
        "        self.vocab_list = list(self.vocab)\n",
        "\n",
        "    def replace_target_words(self):\n",
        "        # Copy docs into a new array where I will replace 50% of target words with psuedo words\n",
        "        self.docs_sampled = copy.deepcopy(self.docs)\n",
        "        # loop over the target words\n",
        "        for target in self.target_words:\n",
        "            indices = []\n",
        "            # Loop over the docs\n",
        "            for i in range(len(self.docs_sampled)):\n",
        "                # get the indices where the target word occurs in the doc\n",
        "                jj = np.where(\n",
        "                    np.array(self.docs_sampled[i]).astype(str) == target)[0]\n",
        "                # Add the indices in the form (doc index, word index)\n",
        "                for j in jj:\n",
        "                    indices.append((i, j))\n",
        "\n",
        "            # Calculate half of the number of occurences, using DIV\n",
        "            half = len(indices) // 2\n",
        "            # Randomly generate a list from the list of indices half the size\n",
        "            samples = random.sample(indices, half)\n",
        "            # For each index to replace replace the target word at that point with the reveresed version\n",
        "            for sample in samples:\n",
        "                self.docs_sampled[sample[0]][sample[1]] = target[::-1]\n",
        "\n",
        "    def contruct_feature_mat(self, context_window: int = 20):\n",
        "        feature_mat = []\n",
        "        # the N dimension is the target and psuedowords\n",
        "        term_vec = self.target_words.copy() + self.psuedo_words.copy()\n",
        "        # combine and flatten all the reviews into one list\n",
        "        flat_sampled_doscs = np.array(\n",
        "            [item for sublist in self.docs_sampled for item in sublist])\n",
        "        # loop over the terms\n",
        "        for term in term_vec:\n",
        "            # instantiate the feature vector as an array of 0s size of the vocab\n",
        "            feature_vec = [0] * len(self.vocab)\n",
        "            # Get all the indices of term in the flat list\n",
        "            indices = np.where(flat_sampled_doscs == term)[0]\n",
        "            # for each occurance of the word find the words in the context\n",
        "            # window around it, and update the feature vector to show a 1\n",
        "            # if the word occurs in the context window\n",
        "            for ind in indices:\n",
        "                # edge case where the window extends past the start of the\n",
        "                # flat list\n",
        "                if ind-context_window < 0:\n",
        "                    context_words = list(\n",
        "                        flat_sampled_doscs[:ind+context_window+1])\n",
        "                else:\n",
        "                    context_words = list(flat_sampled_doscs[ind -\n",
        "                                                            context_window:ind+context_window+1])\n",
        "                # remove middle term, which is the term we are checking the context for\n",
        "                context_words.pop(len(context_words)//2 + 1)\n",
        "                for word in context_words:\n",
        "                    try:\n",
        "                        feature_ind = self.vocab_list.index(word)\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "                    feature_vec[feature_ind] = 1\n",
        "            # add the feature vec to the matrix\n",
        "            feature_mat.append(feature_vec)\n",
        "        # set the feature matrix\n",
        "        self.feature_mat = np.array(feature_mat)\n",
        "\n",
        "    def cluster(self) -> float:\n",
        "        # Single value decomposition, convert spare feature vecture\n",
        "        # into dense feature vecture\n",
        "        u, s, v = svd(self.feature_mat)\n",
        "        k = matrix_rank(self.feature_mat)\n",
        "        U = u[:, :k]\n",
        "        S = np.diag(s)[:k, :k]\n",
        "        V = v[:, :k]\n",
        "        dense_term = np.matmul(U, S)\n",
        "        # Use war Hierarchical clustering with 50 clusters\n",
        "        clustering = AgglomerativeClustering(n_clusters=50).fit(dense_term)\n",
        "        result = list(clustering.labels_)\n",
        "\n",
        "        # Check if the target words are group with their psuedoword\n",
        "        correct = []\n",
        "        for i in range(50):\n",
        "            correct.append(result[i] == result[i+50])\n",
        "\n",
        "        # Accuracy is the number of correctly group target and psuedoword pairs\n",
        "        # divided by the total number of clusters, as a percentage\n",
        "        accuracy = (correct.count(True)/50)*100\n",
        "\n",
        "        return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17aXIO3hAJ-f",
        "outputId": "fd43431c-ef88-45e8-da01-22217d628a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4598\n",
            "9\n",
            "['use', 'ipod', 'phone', 'router', 'camera', 'work', 'just', 'player', 'like', 'great', 'time', 'batteri', 'problem', 'good', 'diaper', 'product', 'zen', 'need', 'comput', 'want', 'realli', 'look', 'featur', 'qualiti', 'easi', 'thing', 'buy', 'micro', 'instal', 'creativ', 'review', 'make', 'better', 'softwar', 'pictur', 'littl', 'sound', 'bag', 'purchas', 'music', 'song', 'did', 'tri', 'connect', 'mp3', 'set', 'bit', 'new', 'lot', 'doe']\n",
            "['esu', 'dopi', 'enohp', 'retuor', 'aremac', 'krow', 'tsuj', 'reyalp', 'ekil', 'taerg', 'emit', 'irettab', 'melborp', 'doog', 'repaid', 'tcudorp', 'nez', 'deen', 'tupmoc', 'tnaw', 'illaer', 'kool', 'rutaef', 'itilauq', 'isae', 'gniht', 'yub', 'orcim', 'latsni', 'vitaerc', 'weiver', 'ekam', 'retteb', 'rawtfos', 'rutcip', 'lttil', 'dnuos', 'gab', 'sahcrup', 'cisum', 'gnos', 'did', 'irt', 'tcennoc', '3pm', 'tes', 'tib', 'wen', 'tol', 'eod']\n",
            "4646\n",
            "Accuracies: [68.0, 66.0, 68.0, 64.0, 68.0, 66.0, 64.0, 70.0, 70.0, 62.0]\n",
            "Mean of Accuracies: 66.6\n",
            "Stand Deviation of Accuracies: 2.5377155080899043\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    dist_sem = DistributionalSemantics()\n",
        "    # specify the directory path to the review files\n",
        "    corpora = dist_sem.read_data(\n",
        "        \"/content/drive/MyDrive/COMP34711-Deep-Learning/product_reviews\")\n",
        "\n",
        "    dist_sem.preprocess(corpora)\n",
        "    print(len(dist_sem.vocab))\n",
        "    print(len(dist_sem.docs))\n",
        "    dist_sem.find_target_words()\n",
        "    print(dist_sem.target_words)\n",
        "    print(dist_sem.psuedo_words)\n",
        "    print(len(dist_sem.vocab))\n",
        "\n",
        "    accuracies = []\n",
        "    for i in range(10):\n",
        "        dist_sem.replace_target_words()\n",
        "        dist_sem.contruct_feature_mat(context_window=20)\n",
        "        accuracies.append(dist_sem.cluster())\n",
        "\n",
        "    print(f\"Accuracies: {accuracies}\")\n",
        "    print(f\"Mean of Accuracies: {np.mean(accuracies)}\")\n",
        "    print(f\"Stand Deviation of Accuracies: {np.std(accuracies)}\")\n",
        "    # TODO: hyper-parameter selection (context window size)\n",
        "\n",
        "\n",
        "test = main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
